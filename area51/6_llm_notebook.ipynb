{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRdXCXGf5xeI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT MLM"
      ],
      "metadata": {
        "id": "kTPB8eoFuL63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer  = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "z3_N2I9P59io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for _ in range(10):\n",
        "    input_ids = torch.randint(0, len(tokenizer), (16f, 256))\n",
        "    data.append(input_ids)\n"
      ],
      "metadata": {
        "id": "vGIungJZ6VPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert.cuda();"
      ],
      "metadata": {
        "id": "XJY0tuwn5ctt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "942uyjNU6Nyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(bert.parameters(), lr=1e-3)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for input_ids in data:\n",
        "    out = bert(input_ids=input_ids.cuda(), labels=input_ids.cuda())\n",
        "    out.loss.backward()\n",
        "    optimizer.step()\n",
        "end = time.time()\n",
        "print((end-start) / 10.0)"
      ],
      "metadata": {
        "id": "ijq0NrPw6AMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Time for 1M steps (in days):\", int(1e6 * (end-start) / 10 / 3600 / 24))"
      ],
      "metadata": {
        "id": "MUReJElg8L_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient accumulation"
      ],
      "metadata": {
        "id": "bMWhF8ZBudz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return (0.5 * x**2).sum()"
      ],
      "metadata": {
        "id": "LbjQcRVwudYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have:\n",
        "\\begin{equation}\n",
        "\\dfrac{\\partial}{\\partial x_i} f(x) = \\dfrac{\\partial}{\\partial x_i}  \\sum_{i=1}^n\\dfrac{1}{2} x_i^2 = x_i.\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "0cEDB_9EvFN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(10, dtype=float, requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "id": "684042sx6CUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(10, dtype=float, requires_grad=True)\n",
        "\n",
        "loss = f(x)\n",
        "loss.backward()\n",
        "print(\"Gradient attached to x:\", x.grad)"
      ],
      "metadata": {
        "id": "5mvRrQWDun3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(10, dtype=float, requires_grad=True)\n",
        "print(\"Initial gradient:\", x.grad)\n",
        "for i in range(2):\n",
        "    loss = f(x)\n",
        "    loss.backward()\n",
        "    input(\"Continue\")\n",
        "    print(f\"Gradient attached to x at step {i+1}:\", x.grad)"
      ],
      "metadata": {
        "id": "HIUCR1jZvZYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A gradient accumulation is readily performed like:"
      ],
      "metadata": {
        "id": "N3ZjSOlSwDFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bert = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer  = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "16_u0A42wCi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "batch_size = 2\n",
        "for _ in range(10):\n",
        "    input_ids = torch.randint(0, len(tokenizer), (batch_size, 8))\n",
        "    data.append(input_ids)"
      ],
      "metadata": {
        "id": "_6cKEWhywLsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert.cuda();"
      ],
      "metadata": {
        "id": "26XD85aYxBbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(bert.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "iteration_steps = 0\n",
        "optimization_steps = 0\n",
        "gradient_accumulation = 2\n",
        "\n",
        "for input_ids in data:\n",
        "    out = bert(input_ids=input_ids.cuda(), labels=input_ids.cuda())\n",
        "    loss = out.loss\n",
        "\n",
        "    loss = loss / gradient_accumulation # To average the gradient, otherwise it performs summation.\n",
        "    loss.backward()\n",
        "\n",
        "    iteration_steps += 1\n",
        "\n",
        "    if (iteration_steps % gradient_accumulation) == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        optimization_steps += 1\n",
        "\n",
        "print(\"Total number of data iterations:\", iteration_steps)\n",
        "print(\"Total number of opimization steps:\", optimization_steps)"
      ],
      "metadata": {
        "id": "7PTgwjSJwOJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2-Large memory requirements\n",
        "\n",
        "Make sure to free the cuda memory before running this (you can relaunch the notebook for instance)."
      ],
      "metadata": {
        "id": "aiZf8Gzr1kSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "xPCuSJwD3Dgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\")\n"
      ],
      "metadata": {
        "id": "pTfaZTpL1jqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = gpt.cuda();\n",
        "num_params = sum(p.numel() for p in gpt.parameters())\n",
        "print(f\"The number of parameters of GPT2-Large is: {num_params}\")"
      ],
      "metadata": {
        "id": "RQXtOyL12CIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "zGQFQkFv2E5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention quadratic\n",
        "relaunch the notebook as well"
      ],
      "metadata": {
        "id": "5aWYS43muzpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "ZIwozvqTva4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_config = GPT2Config(**{\n",
        "  \"activation_function\": \"gelu_new\",\n",
        "  \"architectures\": [\n",
        "    \"GPT2LMHeadModel\"\n",
        "  ],\n",
        "  \"attn_pdrop\": 0.1,\n",
        "  \"bos_token_id\": 1,\n",
        "  \"embd_pdrop\": 0.1,\n",
        "  \"eos_token_id\": 10000,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"layer_norm_epsilon\": 1e-05,\n",
        "  \"model_type\": \"gpt2\",\n",
        "  \"n_ctx\": 1024,\n",
        "  \"n_embd\": 256,\n",
        "  \"n_head\": 4,\n",
        "  \"n_layer\": 6,\n",
        "  \"n_positions\": 8000,\n",
        "  \"resid_pdrop\": 0.1,\n",
        "  \"vocab_size\": 10000\n",
        "})"
      ],
      "metadata": {
        "id": "MnIrouGovbrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = GPT2LMHeadModel(gpt_config)"
      ],
      "metadata": {
        "id": "h_XmrBm4u3RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "list_lengths = [512, 1024, 2048, 4096]\n",
        "for L in list_lengths:\n",
        "    input_ids = torch.randint(0, 10000, (1, L))\n",
        "    data.append(input_ids)"
      ],
      "metadata": {
        "id": "UF6R-_d-u6id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt.cuda();"
      ],
      "metadata": {
        "id": "FFGmFL6gv4ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SAnjW1UjwiNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory_cost = []\n",
        "for input_ids in data:\n",
        "    torch.cuda.empty_cache()\n",
        "    out = gpt(input_ids=input_ids.cuda(), labels=input_ids.cuda())\n",
        "    memory_cost.append(torch.cuda.memory_allocated())\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(list_lengths, memory_cost, linestyle='-')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Input Sequence Length')\n",
        "plt.ylabel('Memory Consumption (MB)')\n",
        "plt.title('GPT2 Memory Consumption vs Input Sequence Length')\n",
        "\n",
        "# Adding grid for better readability\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q_9L4mgZv7Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "pip install evaluate;"
      ],
      "metadata": {
        "id": "ninQhOEc3_fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import evaluate\n"
      ],
      "metadata": {
        "id": "IKkZBV_nwaWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate.load(\"bleu\")"
      ],
      "metadata": {
        "id": "7DBN4IHU4BDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(reference, candidate):\n",
        "    # Tokenize the reference and candidate texts\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = bleu.compute(references=[[reference]], predictions=[candidate])\n",
        "\n",
        "    return bleu_score[\"bleu\"]\n"
      ],
      "metadata": {
        "id": "q-yA2bD4BZ3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage:\n",
        "reference_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "candidate_text = \"The quick pink fox jumps over the sleeping dog.\"\n",
        "bleu_score = compute_bleu(reference_text, candidate_text)\n",
        "print(f\"BLEU Score: {bleu_score}\")"
      ],
      "metadata": {
        "id": "ChKllX3pBhip"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}